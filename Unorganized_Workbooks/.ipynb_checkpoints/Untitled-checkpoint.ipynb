{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard data science imports:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_pushshift(subreddit, epoch, numberofposts = 1000):    \n",
    "    titles = []\n",
    "    subreddits = []\n",
    "    score = []\n",
    "    ids = []\n",
    "    df = pd.DataFrame(columns= ['title','subreddit','score','id'])\n",
    "    headers = {'User-agent': 'JavierM'}\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission/'\n",
    "    params = {\n",
    "        'subreddit':subreddit,\n",
    "        'after':epoch,\n",
    "        'score':'>10',\n",
    "        'size': numberofposts\n",
    "\n",
    "    }\n",
    "\n",
    "    res = requests.get(url, params=params, headers=headers)\n",
    "    if res.status_code == 200:\n",
    "        data = res.json()\n",
    "        print(len(data['data']))\n",
    "        for i in range(len(data['data'])):\n",
    "            titles.append(data['data'][i]['title'])\n",
    "            subreddits.append(data['data'][i]['subreddit'])\n",
    "            score.append(data['data'][i]['score'])\n",
    "            ids.append(data['data'][i]['id'])\n",
    "    else:\n",
    "        print(res.status_code)\n",
    "    time.sleep(1) # input this to not hit the servers so hard\n",
    "    print('DONE!')\n",
    "    df['title'] = titles\n",
    "    df['subreddit'] = subreddits\n",
    "    df['score'] = score\n",
    "    df['id'] = ids\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = ['upliftingnews','news']\n",
    "epochs = ['1451628000','1454306400','1456812000','1459486800','1462078800','1464757200','1467349200','1470027600','1472706000','1475298000','1480572000',' 1480572000','1483250400','1485928800','1488348000','1491022800','1493614800','1496293200','1498885200','1501563600','1504242000','1506834000','1509512400','1512108000','1514786400','1517464800','1519884000','1522558800','1525150800','1527829200','1530421200','1533099600','1535778000','1538370000',' 1541048400','1543644000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfs = pd.DataFrame(columns= ['title','subreddit','score','id'])\n",
    "#for subreddit in subreddits:\n",
    "    #for epoch in epochs:\n",
    "        #frame = get_data_from_pushshift(subreddit, epoch, numberofposts = 1000)\n",
    "        #dfs = pd.concat([dfs,frame],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('2016-2018news&upliftingnews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "news             32012\n",
       "UpliftingNews    18621\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['subreddit_binary'] = df['subreddit'].map(lambda x: 1 if x == 'UpliftingNews' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit_binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015: The Best Year in History for the Average...</td>\n",
       "      <td>UpliftingNews</td>\n",
       "      <td>21</td>\n",
       "      <td>3z0aou</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Deaf man saves deer from frozen river</td>\n",
       "      <td>UpliftingNews</td>\n",
       "      <td>25</td>\n",
       "      <td>3z0u23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Remember that deaf guy who saved and pet the d...</td>\n",
       "      <td>UpliftingNews</td>\n",
       "      <td>2162</td>\n",
       "      <td>3z16kk</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arizona Girl, 12, Collects 1,000 Coats for the...</td>\n",
       "      <td>UpliftingNews</td>\n",
       "      <td>3272</td>\n",
       "      <td>3z3dzy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Russia unveils its body armour for DOGS follow...</td>\n",
       "      <td>UpliftingNews</td>\n",
       "      <td>42</td>\n",
       "      <td>3z3zz0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title      subreddit  score  \\\n",
       "0  2015: The Best Year in History for the Average...  UpliftingNews     21   \n",
       "1              Deaf man saves deer from frozen river  UpliftingNews     25   \n",
       "2  Remember that deaf guy who saved and pet the d...  UpliftingNews   2162   \n",
       "3  Arizona Girl, 12, Collects 1,000 Coats for the...  UpliftingNews   3272   \n",
       "4  Russia unveils its body armour for DOGS follow...  UpliftingNews     42   \n",
       "\n",
       "       id  subreddit_binary  \n",
       "0  3z0aou                 1  \n",
       "1  3z0u23                 1  \n",
       "2  3z16kk                 1  \n",
       "3  3z3dzy                 1  \n",
       "4  3z3zz0                 1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['title']\n",
    "y = df['subreddit_binary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37974, 27915)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "X_train_features = vectorizer.fit_transform(X_train)\n",
    "X_test_features = vectorizer.transform(X_test)\n",
    "\n",
    "X_train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    8.5s\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:   45.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params:  {'model__C': 1, 'model__penalty': 'l2', 'vect__min_df': 4, 'vect__stop_words': None}\n",
      "Best Estimator Score:  0.8572557074018485\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('model', LogisticRegression())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'vect__min_df':[2,4],\n",
    "    'vect__stop_words':[None ,'english'],\n",
    "    'model__penalty':['l1','l2'],\n",
    "    'model__C':[0.1, 1, 10]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, params, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best Params: ',gs.best_params_)\n",
    "\n",
    "print('Best Estimator Score: ', gs.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    8.4s\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:  1.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params:  {'model__C': 1, 'model__penalty': 'l2', 'vect__min_df': 2, 'vect__stop_words': None}\n",
      "Best Estimator Score:  0.8585986254838455\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('model', LogisticRegression())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'vect__min_df':[2,4],\n",
    "    'vect__stop_words':[None ,'english'],\n",
    "    'model__penalty':['l1','l2'],\n",
    "    'model__C':[0.1, 1, 10]  \n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, params, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best Params: ',gs.best_params_)\n",
    "\n",
    "print('Best Estimator Score: ', gs.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    8.1s\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:   45.5s\n",
      "[Parallel(n_jobs=-1)]: Done 341 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed:  4.3min\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('model', RandomForestClassifier() )\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'vect__min_df':[2,4],\n",
    "    'vect__stop_words':[None ,'english'],\n",
    "    'model__n_estimators':[10, 75, 150],\n",
    "    'model__max_depth':[5, 10,15],\n",
    "    'model__min_samples_split':[2,3,4]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, params, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best Params: ',gs.best_params_)\n",
    "\n",
    "print('Best Estimator Score: ', gs.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 180 candidates, totalling 900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   13.1s\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:   35.5s\n",
      "[Parallel(n_jobs=-1)]: Done 341 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 900 out of 900 | elapsed:  3.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params:  {'model__max_depth': 5, 'model__min_samples_split': 2, 'model__n_estimators': 10, 'vect__min_df': 2, 'vect__stop_words': 'english'}\n",
      "Best Estimator Score:  0.6337783395212891\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('model', RandomForestClassifier() )\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'vect__min_df':[2,4],\n",
    "    'vect__stop_words':[None ,'english'],\n",
    "    'model__n_estimators':[10, 20, 30],\n",
    "    'model__max_depth':[1,2,3,4,5],\n",
    "    'model__min_samples_split':[2,3,4]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, params, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best Params: ',gs.best_params_)\n",
    "\n",
    "print('Best Estimator Score: ', gs.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   16.7s\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed: 10.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params:  {'model__base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'model__n_estimators': 200, 'vect__min_df': 2, 'vect__stop_words': None}\n",
      "Best Estimator Score:  0.8123074492455961\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('model', AdaBoostClassifier())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'vect__min_df':[2,4],\n",
    "    'vect__stop_words':[None ,'english'],\n",
    "    'model__base_estimator':[DecisionTreeClassifier(max_depth=1), DecisionTreeClassifier(max_depth=2), DecisionTreeClassifier(max_depth=3),DecisionTreeClassifier(max_depth=4)],\n",
    "    'model__n_estimators':[50, 100, 200]  \n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, params, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best Params: ',gs.best_params_)\n",
    "\n",
    "print('Best Estimator Score: ', gs.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   14.8s\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed:  8.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params:  {'model__base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'model__n_estimators': 200, 'vect__min_df': 4, 'vect__stop_words': None}\n",
      "Best Estimator Score:  0.8236827553519235\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('model', AdaBoostClassifier())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'vect__min_df':[2,4],\n",
    "    'vect__stop_words':[None ,'english'],\n",
    "    'model__base_estimator':[DecisionTreeClassifier(max_depth=1), DecisionTreeClassifier(max_depth=2), DecisionTreeClassifier(max_depth=3),DecisionTreeClassifier(max_depth=4)],\n",
    "    'model__n_estimators':[50, 100, 200]  \n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, params, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best Params: ',gs.best_params_)\n",
    "\n",
    "print('Best Estimator Score: ', gs.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-c55e8f5b4be1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfeats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfeats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'barh'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lr' is not defined"
     ]
    }
   ],
   "source": [
    "feats = pd.DataFrame(lr.coef_, columns=vocab)\n",
    "feats.loc[0,feats.iloc[0,:].abs() >2].plot(kind='barh', figsize=(10,20));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
